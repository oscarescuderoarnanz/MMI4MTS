{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ca1455-1c11-465a-9c5d-dd4812b6d4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Masking, GRU, Dropout, Dense\n",
    "from tensorflow.keras import backend as K\n",
    "import pickle\n",
    "import sklearn \n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "\n",
    "import random, os, json\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "import gc \n",
    "import sys\n",
    "sys.path.append(\"../../../libraries/\")\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27e13ee5-e4b1-4b0f-8d37-d75f341bd61c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-26 11:48:19.730295: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-11-26 11:48:19.770219: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-11-26 11:48:19.771030: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-26 11:48:20.333272: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.13.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3961d7c-4ff9-4827-810d-bbee08513a81",
   "metadata": {},
   "source": [
    "# Mask with optimization and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd74857f-78bd-42a2-8eee-a86d74755740",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mask:\n",
    "    \"\"\"This class allows to fit and interact with dynamic masks.\n",
    "    Attributes:\n",
    "        perturbation (attribution.perturbation.Perturbation):\n",
    "            An object of the Perturbation class that uses the mask to generate perturbations.\n",
    "        device: The device used to work with the torch tensors.\n",
    "        verbose (bool): True is some messages should be displayed during optimization.\n",
    "        random_seed (int): Random seed for reproducibility.\n",
    "        deletion_mode (bool): True if the mask should identify the most impactful deletions.\n",
    "        eps (float): Small number used for numerical stability.\n",
    "        mask_tensor (torch.tensor): The tensor containing the mask coefficients.\n",
    "        T (int): Number of time steps.\n",
    "        N_features (int): Number of features.\n",
    "        hist (torch.tensor): History tensor containing the metrics at different epochs.\n",
    "        task (str): \"classification\" or \"regression\".\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        verbose: bool = False,\n",
    "        random_seed: int = 42,\n",
    "        deletion_mode: bool = False,\n",
    "        eps: float = 1.0e-7,\n",
    "    ):\n",
    "\n",
    "        self.verbose = verbose\n",
    "        self.random_seed = random_seed\n",
    "        self.deletion_mode = deletion_mode\n",
    "        self.eps = eps\n",
    "        self.mask_model  = None\n",
    "        self.T = None\n",
    "        self.N_features = None\n",
    "        self.model = None\n",
    "        self.n_epoch = None\n",
    "        self.loss_function = None\n",
    "\n",
    "        \n",
    "    class FadeMovingAverageWindow(tf.keras.layers.Layer):\n",
    "        def __init__(self, \n",
    "                     w_mask=None,\n",
    "                     window_size=2,\n",
    "                     mask_value=666,\n",
    "                     initial_mask_coeff=1,\n",
    "                     allow_training=False,\n",
    "                     **kwargs) :\n",
    "            self.w_mask = w_mask\n",
    "            self.window_size = window_size\n",
    "            self.mask_value = mask_value\n",
    "            self.initial_mask_coeff = initial_mask_coeff\n",
    "\n",
    "            self.allow_training = allow_training\n",
    "            self.supports_masking = True\n",
    "\n",
    "            assert allow_training or (w_vec is not None), \\\n",
    "                \"ERROR: non-trainable w_vec must be initialized\"\n",
    "            super().__init__(**kwargs)\n",
    "            return\n",
    "\n",
    "        def get_config(self):\n",
    "            config = super().get_config().copy()\n",
    "            config.update({\n",
    "                'w_mask': self.w_mask,\n",
    "                'window_size': self.window_size,\n",
    "                'mask_value': self.mask_value,\n",
    "                'initial_mask_coeff': self.initial_mask_coeff,\n",
    "                'allow_training': self.allow_training,\n",
    "                'supports_masking': self.supports_masking,\n",
    "            })\n",
    "            return config\n",
    "\n",
    "        \n",
    "        def build(self, input_shape):\n",
    "            batch_size, nun_time_steps, num_feats = input_shape\n",
    "            initializer = tf.keras.initializers.glorot_uniform(seed=42)\n",
    "            self.w_mask = self.add_weight(shape=(1, nun_time_steps, num_feats),\n",
    "                                         name='w_mask',\n",
    "                                         initializer=initializer,\n",
    "                                         trainable=self.allow_training,\n",
    "                                         constraint=lambda x: tf.clip_by_value(x, 0, 1))\n",
    "            super().build(input_shape)\n",
    "            return\n",
    "\n",
    "        def call(self, x, mask=None):\n",
    "            # Masking the missing values with 0s\n",
    "            if mask != None:\n",
    "                mask_matrix = tf.transpose((tf.ones([x.shape[2], 1, 1]) * tf.cast(mask, \"float\")), perm=[1, 2, 0])\n",
    "                x_masked = tf.where(mask_matrix == 1.0, x, 0.0)\n",
    "            else:\n",
    "                x_masked = x.copy()\n",
    "\n",
    "\n",
    "            #Creating the filter_coefs of the moving average window\n",
    "            T = x_masked.shape[1]\n",
    "            T_axis = tf.range(1.0, T + 1, delta=1)\n",
    "            T1_tensor = tf.expand_dims(T_axis, axis=[1])\n",
    "            T2_tensor = tf.expand_dims(T_axis, axis=[0])\n",
    "            filter_coefs = (T1_tensor - T2_tensor)\n",
    "            filter_coefs = (filter_coefs >= 0) & (filter_coefs <= self.window_size) \n",
    "            filter_coefs = tf.cast(filter_coefs, tf.float32)\n",
    "            filter_coefs = filter_coefs / tf.transpose(tf.reshape(tf.math.reduce_sum(filter_coefs, axis=1), (1, filter_coefs.shape[0])))\n",
    "\n",
    "            \n",
    "            \n",
    "            #Applying the filter of moving average window\n",
    "            x_avg = tf.linalg.matmul(filter_coefs, x_masked)\n",
    "\n",
    "            # The perturbation is an affine combination of the input and the previous tensor weighted by the mask\n",
    "            x_pert = x_avg + self.w_mask * (x_masked - x_avg)\n",
    "\n",
    "\n",
    "            if mask != None:\n",
    "                x_pert_masked = tf.where(mask_matrix == 1.0, x_pert, self.mask_value * tf.ones_like(x_pert))\n",
    "                return x_pert_masked\n",
    "            else:\n",
    "                return x_pert \n",
    "        \n",
    "    def compute_loss(\n",
    "        mask_model,\n",
    "        T,\n",
    "        N_features, \n",
    "        y_pred_pert, \n",
    "        y_pred,\n",
    "        loss_fn, \n",
    "        error_factor,\n",
    "        reg_factor, \n",
    "        time_reg_factor, \n",
    "        reg_ref\n",
    "    ):    \n",
    "        mask_values = mask_model.get_weights()[0]\n",
    "        mask_sorted = tf.sort(tf.reshape(mask_values, shape=(T * N_features)))\n",
    "        size_reg = tf.math.reduce_mean((reg_ref - mask_sorted) ** 2).numpy()\n",
    "        time_reg = tf.math.reduce_mean(\n",
    "            tf.abs(mask_values[0, 1:T, :] - mask_values[0, :(T-1), :])\n",
    "        )\n",
    "        error = loss_fn(y_pred_pert, y_pred)\n",
    "        loss = error_factor * error + reg_factor * size_reg + time_reg_factor * time_reg                    \n",
    "    \n",
    "        return loss\n",
    "    \n",
    "    def build_model(\n",
    "        initial_mask_coeff, \n",
    "        n_time_steps, \n",
    "        n_features, \n",
    "        mask_value,\n",
    "        window_size,\n",
    "        sigma_max=2.0,\n",
    "        eps=1.0e-7\n",
    "    ):\n",
    "        inputs = tf.keras.Input(shape=(n_time_steps, n_features), name=\"original_input\")\n",
    "        masked = tf.keras.layers.Masking(mask_value=mask_value)(inputs)\n",
    "        perturbed_output = Mask.FadeMovingAverageWindow(w_mask=None,\n",
    "                                                        window_size = window_size,\n",
    "                                                        mask_value = mask_value,\n",
    "                                                        initial_mask_coeff=initial_mask_coeff,\n",
    "                                                        allow_training=True)(masked)\n",
    "        model = tf.keras.Model(inputs=inputs, outputs=[inputs, perturbed_output])\n",
    "        \n",
    "        return model\n",
    "\n",
    "    # Mask Optimization\n",
    "    def fit(\n",
    "        self,\n",
    "        x_train,\n",
    "        x_val,\n",
    "        model,\n",
    "        n_epochs: int = 500,\n",
    "        keep_ratio: float = 0.5,\n",
    "        initial_mask_coeff: float = 0.5,\n",
    "        size_reg_factor_init: float = 0.5,\n",
    "        size_reg_factor_dilation: float = 100,\n",
    "        time_reg_factor: float = 0,\n",
    "        \n",
    "        sigma_max: float = 2.0,\n",
    "        eps: float = 1.0e-7,\n",
    "        window_size=2,\n",
    "        \n",
    "        mask_value = 666,\n",
    "        peek_duration = 5,\n",
    "        min_delta = 0.00001,\n",
    "        learning_rate: float = 1.0e-3,\n",
    "        momentum: float = 0.9,\n",
    "    ):\n",
    "        \"\"\"This method fits a mask to the input X for the black-box function f.\n",
    "        Args:\n",
    "            X: Input matrix (as a T*N_features torch tensor).\n",
    "            f: Black-box (as a map compatible with torch tensors).\n",
    "            target: If the output to approximate is different from f(X), it can be specified optionally.\n",
    "            n_epoch: Number of steps for the optimization.\n",
    "            keep_ratio: Fraction of elements in X that should be kept by the mask (called a in the paper).\n",
    "            initial_mask_coeff: Initial value for the mask coefficient (called lambda_0 in the paper).\n",
    "            size_reg_factor_init: Initial coefficient for the regulator part of the total loss.\n",
    "            size_reg_factor_dilation: Ratio between the final and the initial size regulation factor\n",
    "                (called delta in the paper).\n",
    "            time_reg_factor: Regulation factor for the variation in time (called lambda_a in the paper).\n",
    "            learning_rate: Learning rate for the torch SGD optimizer.\n",
    "            momentum: Momentum for the SGD optimizer.\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.n_epochs = n_epochs\n",
    "        _, self.T, self.N_features = x_train._flat_shapes[0]\n",
    "        \n",
    "        reg_factor = size_reg_factor_init\n",
    "        error_factor = 1 - 2 * self.deletion_mode  # In deletion mode, the error has to be maximized\n",
    "        reg_multiplicator = np.exp(np.log(size_reg_factor_dilation) / n_epochs)\n",
    "        # print(reg_multiplicator)\n",
    "        # Initializing the reference vector used in the size regulator (called r_a in the paper)\n",
    "        reg_ref = tf.zeros(int((1 - keep_ratio) * self.T * self.N_features))\n",
    "        reg_ref = tf.concat(\n",
    "            (reg_ref, tf.ones(self.T * self.N_features - reg_ref.shape[0])), axis=0\n",
    "        )      \n",
    "        # The initial mask model is defined with the initial mask coefficient\n",
    "        mask_model = Mask.build_model(initial_mask_coeff, \n",
    "                                      self.T, self.N_features,\n",
    "                                      window_size=window_size,\n",
    "                                      mask_value=mask_value)\n",
    "        # Instantiate an optimizer.\n",
    "        optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate, momentum=momentum)\n",
    "        # Instantiate a loss function.\n",
    "        loss_fn = tf.keras.losses.BinaryCrossentropy()\n",
    "        \n",
    "        # Run the optimization\n",
    "        hist_train = []\n",
    "        hist_bce_train = []\n",
    "        hist_val = []\n",
    "        hist_bce_val = []\n",
    "        hist_bce_val_peek = []\n",
    "\n",
    "        stop_training = False\n",
    "        flag_peek = False\n",
    "        past_loss_bce = np.inf\n",
    "        min_value = np.inf\n",
    "        counter=0\n",
    "        for epoch in range(n_epochs):\n",
    "            # Iterate over the batches of the dataset.\n",
    "            tf.keras.backend.clear_session()\n",
    "            v_loss_train = []\n",
    "            v_loss_bce_train = []\n",
    "            for step, (x_batch_train, y_batch_train) in enumerate(x_train):\n",
    "                # Open a GradientTape to record the operations run during the forward pass, which enables auto-differentiation.\n",
    "                with tf.GradientTape() as tape:\n",
    "                    # Apply the mask_model to correct the perturbations.\n",
    "                    x_batch, x_batch_pert = mask_model(x_batch_train, training=True)\n",
    "                    \n",
    "                    # Get the predictions using the black-box model\n",
    "                    y_pred_pert = self.model(x_batch_pert, training=False)\n",
    "                    y_pred = self.model(x_batch_train, training=False)\n",
    "                                        \n",
    "                    # Compute the loss value for this batch.\n",
    "                    loss = Mask.compute_loss(mask_model, \n",
    "                                             self.T, self.N_features, \n",
    "                                             y_pred_pert, y_pred, \n",
    "                                             loss_fn, error_factor, reg_factor, time_reg_factor, reg_ref)\n",
    "                    v_loss_train.append(loss)\n",
    "                    \n",
    "                    loss_bce = loss_fn(y_pred_pert, y_pred)                \n",
    "                    v_loss_bce_train.append(loss_bce)\n",
    "                # Use the gradient tape to automatically retrieve\n",
    "                # the gradients of the trainable variables with respect to the loss.\n",
    "                grads = tape.gradient(loss, mask_model.trainable_weights)\n",
    "                # Run one step of gradient descent by updating\n",
    "                # the value of the variables to minimize the loss.\n",
    "                optimizer.apply_gradients(zip(grads, mask_model.trainable_weights))\n",
    "                # Memory cleanup after processing the batch\n",
    "                del x_batch_train, y_batch_train\n",
    "                gc.collect()  # Force garbage collection\n",
    "            \n",
    "            v_loss_val = []\n",
    "            v_loss_bce_val = []\n",
    "            for step, (x_batch_val, y_batch_val) in enumerate(x_val):\n",
    "                # Apply the mask_model to correct the perturbations.\n",
    "                x_batch, x_batch_pert = mask_model(x_batch_val, training=False)\n",
    "                # Get the predictions using the black-box model\n",
    "                y_pred_pert = self.model(x_batch_pert, training=False)\n",
    "                y_pred = self.model(x_batch_val, training=False)\n",
    "                \n",
    "                # Compute the loss value for this batch.\n",
    "                loss = Mask.compute_loss(mask_model, \n",
    "                                         self.T, self.N_features, \n",
    "                                         y_pred_pert, y_pred, \n",
    "                                         loss_fn, error_factor, reg_factor, time_reg_factor, reg_ref)\n",
    "                v_loss_val.append(loss)\n",
    "                \n",
    "                loss_bce = loss_fn(y_pred_pert, y_pred)                \n",
    "                v_loss_bce_val.append(loss_bce)\n",
    "                # Memory cleanup after validation\n",
    "                del x_batch_val, y_batch_val\n",
    "                gc.collect()  # Force garbage collection\n",
    "                \n",
    "            self.mask_model = mask_model\n",
    "\n",
    "            # Memory cleanup and garbage collection after training and validation\n",
    "            gc.collect()  # Force garbage collection\n",
    "            tf.keras.backend.clear_session()  # Clear session to free up memory\n",
    "\n",
    "            stop_training, flag_peek,  past_loss_bce, min_value, hist_bce_val, hist_bce_val_peek, counter = check_early_stopping(\n",
    "                np.array(v_loss_bce_val).mean(), past_loss_bce, min_value, hist_bce_val, hist_bce_val_peek,\n",
    "                flag_peek, peek_duration, counter, min_delta,\n",
    "                mask_model\n",
    "            )\n",
    "            \n",
    "            if stop_training:\n",
    "                break\n",
    "            \n",
    "            # print(\"reg_factor\", reg_factor)\n",
    "            reg_factor *= reg_multiplicator\n",
    "            \n",
    "            hist_train.append(np.array(v_loss_train).mean())\n",
    "            hist_val.append(np.array(v_loss_val).mean())\n",
    "            hist_bce_train.append(np.array(v_loss_bce_train).mean())     \n",
    "        \n",
    "        return np.array(hist_train), np.array(hist_val), np.array(hist_bce_train), np.array(hist_bce_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db6d256-89c3-4619-ab25-df292537145d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_early_stopping(\n",
    "    loss_bce, past_loss_bce, min_value, v_loss_bce, v_loss_bce_peek,\n",
    "    flag_peek, peek_duration, counter, min_delta,\n",
    "    mask_model\n",
    "):\n",
    "    stop_training = False\n",
    "    if not flag_peek:\n",
    "        v_loss_bce.append(loss_bce) \n",
    "        diff = past_loss_bce - loss_bce\n",
    "        if (diff < 0) and (abs(diff) > min_delta):\n",
    "            flag_peek = True\n",
    "        else:\n",
    "            past_loss_bce = loss_bce\n",
    "            if loss_bce < min_value:\n",
    "                min_value = loss_bce\n",
    "    else:\n",
    "        if counter < peek_duration:\n",
    "            v_loss_bce_peek.append(loss_bce)\n",
    "            counter += 1\n",
    "        else:\n",
    "            flag_peek = False\n",
    "            best_value = v_loss_bce[-2]  \n",
    "            minimum_peek_value = min(v_loss_bce_peek)\n",
    "            if minimum_peek_value < best_value:\n",
    "                index = v_loss_bce_peek.index(minimum_peek_value)\n",
    "                v_loss_bce.extend(v_loss_bce_peek[:index + 1])  \n",
    "                if minimum_peek_value < min_value:\n",
    "                    min_value = minimum_peek_value\n",
    "                counter = 0\n",
    "                v_loss_bce_peek.clear()\n",
    "                gc.collect()  \n",
    "            else:\n",
    "                stop_training = True\n",
    "\n",
    "    return stop_training, flag_peek, past_loss_bce, min_value, v_loss_bce, v_loss_bce_peek, counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3973a7d-0775-4ecb-a7cc-72f4ca655a06",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970f8b6f-2992-40ca-b80b-24491bf5ae1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds = [20, 30, 45, 70]\n",
    "\n",
    "n_epochs = 500\n",
    "size_reg_factor_dilation = n_epochs / 4\n",
    "\n",
    "learning_rate = [0.0001, 0.001, 0.01, 0.1]\n",
    "keep_ratio = [0.2, 0.4, 0.6]\n",
    "size_reg_factor_init = [0.2, 0.4, 0.6]\n",
    "\n",
    "w2 = 0.18\n",
    "w1 = 0.82\n",
    "\n",
    "inputShape = 56\n",
    "n_time_steps = 14\n",
    "batch_size = 32\n",
    "epochs = 500\n",
    "patience = 10\n",
    "\n",
    "hyperparameters = {\n",
    "    \"epochs\": epochs,\n",
    "    \"n_dynamic_features\": inputShape,\n",
    "    \"n_timesteps\": n_time_steps,\n",
    "    'batch_size': batch_size,\n",
    "    'patience':10,\n",
    "    \"w1\":w1, \"w2\":w2, \n",
    "    'mask_value':666,\n",
    "    'monitor': 'val_loss', \"mindelta\": 0,\n",
    "    'verbose':0,\n",
    "    'level':3\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42c600d-5671-4469-8f50-90832643cdad",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad88203-ef3b-4d3d-9f0a-a5418212011a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "logging.basicConfig(\n",
    "    filename='dynamask.txt', \n",
    "    filemode='a',                 \n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    level=logging.INFO             \n",
    ")\n",
    "\n",
    "def load_data(split, fold):\n",
    "    X_train = np.load(f\"../../../ORIGINAL_DATA/MDR/splits_14_days/notbalanced/split_{split}/X_train_tensor_{fold}.npy\")\n",
    "    y_train = pd.read_csv(f\"../../../ORIGINAL_DATA/MDR/splits_14_days/notbalanced/split_{split}/y_train_{fold}.csv\", index_col=0)\n",
    "    X_val = np.load(f\"../../../ORIGINAL_DATA/MDR/splits_14_days/notbalanced/split_{split}/X_val_tensor_{fold}.npy\")\n",
    "    y_val = pd.read_csv(f\"../../../ORIGINAL_DATA/MDR/splits_14_days/notbalanced/split_{split}/y_val_{fold}.csv\", index_col=0)\n",
    "    return X_train, y_train, X_val, y_val\n",
    "\n",
    "def myCVGrid(hyperparameters, learning_rate, keep_ratio, size_reg_factor_init, size_reg_factor_dilation, model_GRU, split, seed):\n",
    "    \n",
    "    best_hyperparameters = {}\n",
    "    lowest_mean_val_loss = np.inf  \n",
    "\n",
    "    logging.info(\"Starting the search...\")  \n",
    "\n",
    "    log_interval = 5  \n",
    "    iteration_count = 0\n",
    "\n",
    "    for i in range(len(learning_rate)):\n",
    "        for j in range(len(keep_ratio)):\n",
    "            for k in range(len(size_reg_factor_init)):\n",
    "                    \n",
    "                iteration_count += 1\n",
    "                if iteration_count % log_interval == 0:\n",
    "                    logging.info(f\"Still running... Evaluated {iteration_count} combinations.\")  \n",
    "\n",
    "                hyperparameters_copy = hyperparameters.copy()\n",
    "                hyperparameters_copy['learning_rate'] = learning_rate[i]\n",
    "                hyperparameters_copy['keep_ratio'] = keep_ratio[j]\n",
    "                hyperparameters_copy['size_reg_factor_init'] = size_reg_factor_init[k]\n",
    "\n",
    "\n",
    "                total_val_loss = 0.0\n",
    "\n",
    "                for fold in range(5):\n",
    "                    X_train, y_train, X_val, y_val = load_data(split, fold)\n",
    "\n",
    "                    train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(1024).batch(32)\n",
    "                    val_dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val)).shuffle(1024).batch(32)\n",
    "\n",
    "                    mask = Mask()  \n",
    "                    _, _, _, bce_val = mask.fit(\n",
    "                        train_dataset, val_dataset,\n",
    "                        model_GRU,\n",
    "                        n_epochs=hyperparameters_copy['epochs'],\n",
    "                        learning_rate=hyperparameters_copy['learning_rate'],\n",
    "                        peek_duration=4,\n",
    "                        min_delta=hyperparameters_copy['mindelta'],\n",
    "                        keep_ratio=hyperparameters_copy['keep_ratio'],\n",
    "                        size_reg_factor_init=hyperparameters_copy['size_reg_factor_init'],\n",
    "                        size_reg_factor_dilation=size_reg_factor_dilation,\n",
    "                        time_reg_factor=0,\n",
    "                        window_size=5\n",
    "                    )\n",
    "\n",
    "                    total_val_loss += bce_val.min() \n",
    "\n",
    "                    del X_train, y_train, X_val, y_val, train_dataset, val_dataset\n",
    "                    gc.collect()  \n",
    "\n",
    "                mean_val_loss = total_val_loss / 5 \n",
    "\n",
    "                if mean_val_loss < lowest_mean_val_loss:\n",
    "                    lowest_mean_val_loss = mean_val_loss\n",
    "                    best_hyperparameters = {\n",
    "                        'learning_rate': learning_rate[i],\n",
    "                        'keep_ratio': keep_ratio[j],\n",
    "                        'size_reg_factor_init': size_reg_factor_init[k],\n",
    "                    }\n",
    "\n",
    "                # Garbage collection \n",
    "                gc.collect()\n",
    "\n",
    "    return best_hyperparameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dcb672f-ce66-4a28-b059-e21791ac597f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "def load_keras_model(filepath):\n",
    "    custom_loss = utils.weighted_binary_crossentropy(hyperparameters)\n",
    "    return load_model(filepath, custom_objects={'loss': custom_loss})\n",
    "\n",
    "run_model = False\n",
    "n = 4\n",
    "if run_model:\n",
    "    v_early = []\n",
    "    loss_dev = []\n",
    "    v_models = []\n",
    "    bestHyperparameters_bySplit = {}\n",
    "    y_pred_by_split = {}\n",
    "    weights = []\n",
    "\n",
    "    for i in [1, 2, 3]:\n",
    "        X_test = np.load(\"../../../ORIGINAL_DATA/MDR/splits_14_days/notbalanced/split_\" + str(i) + \"/X_test_tensor.npy\")\n",
    "        y_test = pd.read_csv(\"../../../ORIGINAL_DATA/MDR/splits_14_days/notbalanced/split_\" + str(i) + \"/y_test.csv\",\n",
    "                            index_col=0)\n",
    "        \n",
    "        X_train = np.load(\"../../../ORIGINAL_DATA/MDR/splits_14_days/notbalanced/split_\" + str(i) +\n",
    "                              \"/X_train_tensor_\" + str(n)+ \".npy\")\n",
    "        y_train = pd.read_csv(\"../../../ORIGINAL_DATA/MDR/splits_14_days/notbalanced/split_\" + str(i) +\n",
    "                              \"/y_train_\" + str(n)+ \".csv\",\n",
    "                             index_col=0)\n",
    "\n",
    "        X_val = np.load(\"../../../ORIGINAL_DATA/MDR/splits_14_days/notbalanced/split_\" + str(i) +\n",
    "                            \"/X_val_tensor_\" + str(n)+ \".npy\")\n",
    "        y_val = pd.read_csv(\"../../../ORIGINAL_DATA/MDR/splits_14_days/notbalanced/split_\" + str(i) +\n",
    "                            \"/y_val_\" + str(n)+ \".csv\",\n",
    "                           index_col=0)\n",
    "\n",
    "\n",
    "        # We take for each split, the best model that we obtained in 0_WITHOUT_FS\n",
    "        model_GRU = load_keras_model(os.path.join(f'../../../experiments/MDR/considering_all_features/Results_GRU/split_{i}/', f\"model_split_{i}.h5\"))\n",
    "        \n",
    "        best_hyperparameters = myCVGrid(hyperparameters, \n",
    "                                        learning_rate, \n",
    "                                        keep_ratio, \n",
    "                                        size_reg_factor_init, \n",
    "                                        size_reg_factor_dilation,\n",
    "                                        model_GRU, \n",
    "                                        i,\n",
    "                                        seeds[i])\n",
    "\n",
    "                \n",
    "        bestHyperparameters_bySplit[str(i)] = best_hyperparameters\n",
    "    \n",
    "        # Save best hyperparameters for current split\n",
    "        split_directory = './Results_Dynamask/split_' + str(i)\n",
    "        if not os.path.exists(split_directory):\n",
    "            os.makedirs(split_directory)\n",
    "    \n",
    "        with open(os.path.join(split_directory, f\"bestHyperparameters_split_{i}.pkl\"), 'wb') as f:\n",
    "            pickle.dump(best_hyperparameters, f)\n",
    "    \n",
    "    \n",
    "        hyperparameters.update({\n",
    "            'learning_rate': best_hyperparameters[\"learning_rate\"],\n",
    "            'keep_ratio': best_hyperparameters[\"keep_ratio\"],\n",
    "            'size_reg_factor_init': best_hyperparameters[\"size_reg_factor_init\"],\n",
    "        })\n",
    "    \n",
    "\n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(1024).batch(32)\n",
    "        val_dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val)).shuffle(1024).batch(32)\n",
    "        \n",
    "        mask = Mask()  \n",
    "        hist_train, hist_val, bce_train, bce_val = mask.fit(\n",
    "            train_dataset, val_dataset,\n",
    "            model_GRU,\n",
    "            n_epochs=hyperparameters['epochs'],\n",
    "            learning_rate=hyperparameters['learning_rate'],\n",
    "            peek_duration=4,\n",
    "            min_delta=hyperparameters['mindelta'],\n",
    "            keep_ratio=hyperparameters['keep_ratio'],\n",
    "            size_reg_factor_init=hyperparameters['size_reg_factor_init'],\n",
    "            size_reg_factor_dilation=size_reg_factor_dilation,\n",
    "            time_reg_factor=0,\n",
    "            window_size=5\n",
    "        )\n",
    "        mask_weights = mask.mask_model.get_weights()[0][0, :, :]\n",
    "        with open(os.path.join(split_directory, f\"weights_split_{i}.pkl\"), 'wb') as f:\n",
    "            pickle.dump(mask_weights, f) \n",
    "    \n",
    "        y_pred = model_GRU.predict(x=[X_test])\n",
    "        y_pred_by_split[str(i)] = y_pred\n",
    "        \n",
    "        with open(os.path.join(split_directory, f\"y_pred_split_{i}.pkl\"), 'wb') as f:\n",
    "            pickle.dump(y_pred, f)\n",
    "    \n",
    "        # Calculate metrics\n",
    "        metrics_dict = utils.calculate_and_save_metrics(\n",
    "        y_test.individualMRGerm.values, \n",
    "        y_pred, \n",
    "        split_directory, \n",
    "        split_index=i\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ba482e-374a-432a-82a7-61de5533c403",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import rcParams\n",
    "rcParams['font.size'] = 38\n",
    "\n",
    "keys = [\n",
    "    'AMG', 'ATF', 'CAR', 'CF1', 'CF2', 'CF3', 'CF4', 'Others', 'GCC', 'GLI', 'LIN', 'LIP', 'MAC',\n",
    "    'MON', 'NTI',  'OTR', 'OXA', 'PAP', 'PEN', 'POL', 'QUI', 'SUL', 'TTC',\n",
    "    \n",
    "    r'$acinet._{pc}$', r'$enterob._{pc}$', r'$enteroc._{pc}$', \n",
    "    r'$pseudo._{pc}$', r'$staph._{pc}$', r'$stenot._{pc}$', r'$others_{pc}$',\n",
    "    \n",
    "    'Mech. Vent.',\n",
    "\n",
    "    '# pat.', '# MDR pat.',\n",
    "    \n",
    "    r'$AMG_{n}$', r'$ATF_{n}$', r'$CAR_{n}$', r'$CF1_{n}$', r'$CF2_{n}$', r'$CF3_{n}$',\n",
    "    r'$CF4_{n}$', r'$Others_{n}$', r'$GCC_{n}$', r'$GLI_{n}$', r'$LIN_{n}$', r'$LIP_{n}$', r'$MAC_{n}$',\n",
    "    r'$MON_{n}$', r'$NTI_{n}$', r'$OTR_{n}$', r'$OXA_{n}$', r'$PAP_{n}$',\n",
    "    r'$PEN_{n}$', r'$POL_{n}$', r'$QUI_{n}$', r'$SUL_{n}$', r'$TTC_{n}$'   \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46128e29-9dde-4708-a13a-2781cf580e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def load_from_pickle(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "for i in [1, 2, 3]:\n",
    "    directory = f'./Results_Dynamask/split_{i}'\n",
    "    mask_weights = load_from_pickle(os.path.join(directory, f\"weights_split_{i}.pkl\"))\n",
    "    \n",
    "    plt.subplots(figsize=(50, 20))\n",
    "    heatmap = sns.heatmap(mask_weights,  cmap='viridis')\n",
    "    heatmap.axes.set_xticklabels(keys, rotation=90)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('./Figures/heatmap_Dynamask_s'+str(i)+'.pdf', bbox_inches='tight', pad_inches=0)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72054efa-3963-4b08-ae43-883b221ab369",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e53fd1-cad7-4788-bbf2-005dfd418394",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
