{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a3db6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "import pickle\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../../../libraries/\")\n",
    "import utils\n",
    "import TFT_library_masking as TFT\n",
    "\n",
    "sys.path.append(\"../../../classification_architectures/\")\n",
    "import fhsi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3d4acb-a2d7-4663-ace5-c67b6228798d",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997830ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_static_data(df_static, keys, categorical_keys):\n",
    "    # Eliminate the non-interesting features and get only one sample per patient\n",
    "    df_static = df_static[keys].drop_duplicates()\n",
    "    \n",
    "    # Fill the NaNs\n",
    "    if \"SAPSIIIScore\" in df_static:\n",
    "        df_static[\"SAPSIIIScore\"] = df_static.SAPSIIIScore.fillna(df_static.SAPSIIIScore.mean())\n",
    "\n",
    "    # Reduce the dimensionality of two categorical features\n",
    "    if 'Origin' in df_static:\n",
    "        booleanMapping = (df_static.Origin == \"rehabilitation\") | (df_static.Origin == \"hemodynamics\") | \\\n",
    "        (df_static.Origin == \"cma\") | (df_static.Origin == \"ICU\")  | (df_static.Origin == \"paediatrics\")  | \\\n",
    "        (df_static.Origin == \"obstetrics\")  | (df_static.Origin == \"anaesthesia\")  | (df_static.Origin == \"other floor\") | \\\n",
    "         (df_static.Origin == \"gynaecology\") | (df_static.Origin == \"psychiatry\") | (df_static.Origin == \"dermatology\")\n",
    "        df_static.Origin = df_static.Origin.where(~(booleanMapping), \"others\")\n",
    "    if 'ReasonAdmission' in df_static:\n",
    "        booleanMapping = (df_static.ReasonAdmission == \"endocrine other\") | (df_static.ReasonAdmission == \"coagulopat√≠a\") | \\\n",
    "        (df_static.ReasonAdmission == \"obstetric pathology\") | (df_static.ReasonAdmission == \"infection other\")  | \\\n",
    "        (df_static.ReasonAdmission == \"other\")  | (df_static.ReasonAdmission == \"hydroelectrolytic alteration\")  | \\\n",
    "        (df_static.ReasonAdmission == \"respiratory other\")  | (df_static.ReasonAdmission == \"severe trauma\") | \\\n",
    "         (df_static.ReasonAdmission == \"hepatic insufficiency\") | (df_static.ReasonAdmission == \"diabetic decompensation\") | \\\n",
    "        (df_static.ReasonAdmission == \"neuromuscular\") | (df_static.ReasonAdmission == \"severe arrhythmia\")\n",
    "        df_static.ReasonAdmission = df_static.ReasonAdmission.where(~(booleanMapping), \"others\")\n",
    "    \n",
    "    # Convert the categories of categorical features into numbers\n",
    "    for i in range(len(categorical_keys)):\n",
    "        df_static[categorical_keys[i]] = pd.factorize(df_static[categorical_keys[i]])[0] + 1\n",
    "        \n",
    "    return df_static\n",
    "\n",
    "df_MR = pd.read_csv(\"../../../ORIGINAL_DATA/MDR/df_estaticas_v2_covid.csv\", low_memory=False)\n",
    "\n",
    "keys = [\n",
    "    'Admissiondboid', \n",
    "    'Age', 'Gender','Origin', 'ReasonAdmission', 'PatientCategory',\n",
    "    'SAPSIIIScore',          \n",
    "    'MonthOfAdmission', 'YearOfAdmission'\n",
    "       ]\n",
    "categorical_keys = [\"Origin\", \"ReasonAdmission\", \"PatientCategory\"]\n",
    "\n",
    "\n",
    "df_static = load_static_data(df_MR, keys, categorical_keys)\n",
    "df_static.head()\n",
    "\n",
    "category_counts = []\n",
    "categorical_features = ['Origin', 'ReasonAdmission', 'PatientCategory']\n",
    "for i in range(len(categorical_features)):\n",
    "    category_counts.append(np.unique(df_static[categorical_features[i]], return_counts=True)[0].shape[0] + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131d40f3",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0665f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds = [20, 30, 45, 70]\n",
    "\n",
    "n_categorical_features = 3\n",
    "n_numerical_features = 5\n",
    "n_static_features = n_categorical_features + n_numerical_features\n",
    "n_dynamic_features = 56\n",
    "n_timesteps = 14\n",
    "\n",
    "# Hyperparamas of network\n",
    "balance = True\n",
    "epochs = 10000\n",
    "batch_size = 128\n",
    "num_heads = 1\n",
    "\n",
    "layers = [3, 5, 8, 10, 15, 20, 25, 30, 35, 40, 50]\n",
    "lr_scheduler = [0.0001, 0.001, 0.01, 0.1]\n",
    "dropout_rate = [0.0, 0.1, 0.2, 0.3]\n",
    "\n",
    "w2 = 0.18\n",
    "w1 = 0.82\n",
    "\n",
    "tensor = True\n",
    "debug = True\n",
    "\n",
    "hyperparameters = {\n",
    "    \"n_categorical_features\": n_categorical_features,\n",
    "    \"n_numerical_features\": n_numerical_features,\n",
    "    \"n_static_features\": n_static_features,\n",
    "    \"n_dynamic_features\": n_dynamic_features,\n",
    "    \"n_timesteps\": n_timesteps,\n",
    "    \"num_heads\": num_heads,\n",
    "    \"w1\":w1, \"w2\":w2, \n",
    "\n",
    "    \"category_counts\": category_counts,\n",
    "    \"epochs\":epochs,\n",
    "    'batch_size': batch_size,\n",
    "    'maskValue':666,\n",
    "    'monitor': 'val_loss', \n",
    "    \"mindelta\": 0,\n",
    "    \"patience\":30,\n",
    "    'balance': balance,\n",
    "    'optimizer':'adam',\n",
    "    'kfold':5,\n",
    "    'level':3, \n",
    "    'verbose':0\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0380775",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9bfcda1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "v_early = []\n",
    "loss_dev = []\n",
    "v_models = []\n",
    "bestHyperparameters_bySplit = {}\n",
    "y_pred_by_split = {}\n",
    "\n",
    "for i in range(4):\n",
    "    \n",
    "    path = f'../../../ORIGINAL_DATA/MDR/splits_14_days/notbalanced/split_{str(i)}/'\n",
    "\n",
    "    X_test_dynamic = np.load(path + f\"/X_test_tensor.npy\")\n",
    "    X_test_static = pd.read_csv(path + f\"/X_test_static.csv\", index_col=0)\n",
    "    y_test = pd.read_csv(path + f\"/y_test.csv\", index_col=0)\n",
    "\n",
    "    bestHyperparameters, X_train_dyn, y_train, X_train_static, X_val_dyn, y_val, X_val_static = fhsi.myCVGrid(hyperparameters,\n",
    "                                                                                                 dropout_rate,\n",
    "                                                                                                 lr_scheduler,\n",
    "                                                                                                 layers,\n",
    "                                                                                                 i,                                                              \n",
    "                                                                                                 seeds[i],\n",
    "                                                                                                 path\n",
    "                                                                                                 )\n",
    "    bestHyperparameters_bySplit[str(i)] = bestHyperparameters\n",
    "    \n",
    "\n",
    "    # Save best hyperparameters for current split\n",
    "    split_directory = './Results_FHSI/split_' + str(i)\n",
    "    if not os.path.exists(split_directory):\n",
    "        os.makedirs(split_directory)\n",
    "\n",
    "    with open(os.path.join(split_directory, f\"bestHyperparameters_split_{i}.pkl\"), 'wb') as f:\n",
    "        pickle.dump(bestHyperparameters, f)\n",
    "\n",
    "\n",
    "    hyperparameters = {\n",
    "        \"n_categorical_features\": hyperparameters[\"n_categorical_features\"],\n",
    "        \"n_numerical_features\": hyperparameters[\"n_numerical_features\"],\n",
    "        \"n_static_features\": hyperparameters[\"n_static_features\"],\n",
    "        \"n_dynamic_features\": hyperparameters[\"n_dynamic_features\"],\n",
    "        \"w1\":hyperparameters[\"w1\"], \"w2\":hyperparameters[\"w2\"],                                    \n",
    "\n",
    "        \"n_timesteps\": hyperparameters[\"n_timesteps\"],\n",
    "        \"category_counts\": hyperparameters[\"category_counts\"],\n",
    "        'epochs':  hyperparameters[\"epochs\"],\n",
    "        \"num_heads\": num_heads,\n",
    "        'batch_size': hyperparameters[\"batch_size\"],\n",
    "        'maskValue': hyperparameters[\"maskValue\"],\n",
    "        'earlyStopping': True,\n",
    "        'kfold': hyperparameters[\"kfold\"],\n",
    "        'monitor': hyperparameters[\"monitor\"],\n",
    "        \"mindelta\": hyperparameters[\"mindelta\"],\n",
    "        \"patience\": hyperparameters[\"patience\"],\n",
    "        'balance': hyperparameters[\"balance\"],\n",
    "        \"dropout_rate\": bestHyperparameters[\"dropout_rate\"],\n",
    "        \"layers\": bestHyperparameters[\"layers\"],\n",
    "        \"lr_scheduler\": bestHyperparameters[\"lr_scheduler\"],\n",
    "        \"level\": 3, 'verbose': 0\n",
    "    }\n",
    "\n",
    "    #Try on test\n",
    "    utils.reset_keras()\n",
    "\n",
    "    model, hist, early = fhsi.run_network(\n",
    "        X_train_dyn, X_train_static, y_train.individualMRGerm.values,\n",
    "        X_val_dyn, X_val_static, y_val.individualMRGerm.values,\n",
    "        hyperparameters, \n",
    "        seeds[i]\n",
    "    )    \n",
    "\n",
    "    v_models.append(model)\n",
    "    loss_dev.append(hist.history['val_loss'])\n",
    "\n",
    "    y_pred = model.predict(x=[X_test_static.values, X_test_dynamic])\n",
    "    y_pred_by_split[str(i)] = y_pred\n",
    "    \n",
    "    with open(os.path.join(split_directory, f\"y_pred_split_{i}.pkl\"), 'wb') as f:\n",
    "        pickle.dump(y_pred, f)\n",
    "\n",
    "    # Save model for current split\n",
    "    model_filename = os.path.join(split_directory, f\"model_split_{i}.h5\")\n",
    "    model.save(model_filename)\n",
    "\n",
    "    # Calculate metrics\n",
    "    metrics_dict = utils.calculate_and_save_metrics(\n",
    "    y_test.individualMRGerm.values, \n",
    "    y_pred, \n",
    "    split_directory, \n",
    "    split_index=i\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4338b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2baa076",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
