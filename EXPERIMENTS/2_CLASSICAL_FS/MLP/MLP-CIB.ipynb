{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f52cfa-bf42-4e2c-8674-ebe50a8aeb28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sklearn\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import pickle\n",
    "\n",
    "import logging\n",
    "tf.get_logger().setLevel(logging.ERROR)\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../../../Libraries/\")\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3969804b-d4a9-487a-93ac-4b9452e6e617",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da680260-8950-47a5-a8c0-d2cba2940e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hyperparameters):\n",
    "    # static preprocessing.\n",
    "    static_input = tf.keras.layers.Input(shape=(hyperparameters[\"n_static_features\"]))\n",
    "    hidden_layer = tf.keras.layers.Dense(\n",
    "        hyperparameters[\"layers\"],\n",
    "        activation='tanh'\n",
    "    )(static_input)\n",
    "    dp_layer = tf.keras.layers.Dropout(hyperparameters[\"dropout_rate\"], noise_shape=None, seed=42)(hidden_layer)\n",
    "    \n",
    "    # Concatenation\n",
    "    output = tf.keras.layers.Dense(1, activation=\"sigmoid\")(dp_layer)\n",
    "    \n",
    "    model = tf.keras.Model([static_input], [output])\n",
    "    customized_loss = utils.weighted_binary_crossentropy(hyperparameters)\n",
    "    myOptimizer = myOptimizer = tf.keras.optimizers.Adam(learning_rate=hyperparameters[\"lr_scheduler\"])\n",
    "    model.compile(loss=customized_loss, optimizer=myOptimizer)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69702b44-5576-486c-8b1c-af515a9cbda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_network(X_train, y_train,\n",
    "                X_val, y_val,\n",
    "                hyperparameters, seed):\n",
    "    model = None\n",
    "    model = build_model(hyperparameters)\n",
    "    try:\n",
    "        earlystopping = tf.keras.callbacks.EarlyStopping(\n",
    "            monitor=hyperparameters[\"monitor\"],\n",
    "            min_delta=hyperparameters[\"mindelta\"],\n",
    "            patience=hyperparameters[\"patience\"],\n",
    "            restore_best_weights=True,\n",
    "            mode=\"min\"\n",
    "        )\n",
    "\n",
    "        hist = model.fit(\n",
    "            x=[X_train], y=y_train,\n",
    "            validation_data=([X_val], y_val),\n",
    "            callbacks=[earlystopping], batch_size=hyperparameters['batch_size'], epochs=hyperparameters['epochs'],\n",
    "            verbose=hyperparameters['verbose']\n",
    "        )\n",
    "\n",
    "        return model, hist, earlystopping\n",
    "    except KeyboardInterrupt:\n",
    "        print ('Training duration (s) : ', time.time() - global_start_time)\n",
    "        return model, y_test, 0, 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a264e83c-22cc-47d3-b77d-05187bf42e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def myCVGrid(hyperparameters, dropout, lr_scheduler, layers, split, seed):\n",
    "    bestHyperparameters = {}\n",
    "    bestMetricDev = np.inf\n",
    "\n",
    "    for k in range(len(dropout)):\n",
    "        for l in range(len(layers)):\n",
    "            for m in range(len(lr_scheduler)):\n",
    "                v_early = []\n",
    "                v_metric_dev = []\n",
    "                v_hist = []\n",
    "                v_val_loss = []\n",
    "\n",
    "                hyperparameters_copy = hyperparameters.copy()\n",
    "                hyperparameters_copy['dropout_rate'] = dropout[k]\n",
    "                hyperparameters_copy['layers'] = layers[l]\n",
    "                hyperparameters_copy['lr_scheduler'] = lr_scheduler[m]\n",
    "                \n",
    "                for n in range(5):\n",
    "\n",
    "                    X_train = pd.read_csv(\"../../../ORIGINAL_DATA/splits_14_days/bootstrap/split_\" + str(i) +\n",
    "                                             \"/X_train_static_\" + str(n)+ \".csv\", index_col=0)\n",
    "                    y_train = pd.read_csv(\"../../../ORIGINAL_DATA/splits_14_days/bootstrap/split_\" + str(i) +\n",
    "                                          \"/y_train_\" + str(n)+ \".csv\",\n",
    "                                         index_col=0)\n",
    "                    \n",
    "                    X_val = pd.read_csv(\"../../../ORIGINAL_DATA/splits_14_days/bootstrap/split_\" + str(i) +\n",
    "                                             \"/X_val_static_\" + str(n)+ \".csv\", index_col=0)\n",
    "                    y_val = pd.read_csv(\"../../../ORIGINAL_DATA/splits_14_days/bootstrap/split_\" + str(i) +\n",
    "                                        \"/y_val_\" + str(n)+ \".csv\",\n",
    "                                       index_col=0)\n",
    "\n",
    "                    utils.reset_keras()\n",
    "                    model, hist, early = run_network(\n",
    "                        X_train, \n",
    "                        y_train,\n",
    "                        X_val, \n",
    "                        y_val,\n",
    "                        hyperparameters_copy,  \n",
    "                        seed\n",
    "                    )\n",
    "                                        \n",
    "                    v_early.append(early)\n",
    "                    v_hist.append(hist)\n",
    "                    v_val_loss.append(np.min(hist.history[\"val_loss\"]))\n",
    "                    \n",
    "                metric_dev = np.mean(v_val_loss)\n",
    "                if metric_dev < bestMetricDev:\n",
    "                    bestMetricDev = metric_dev\n",
    "                    bestHyperparameters = {\n",
    "                        'dropout_rate': dropout[k],\n",
    "                        'layers': layers[l],\n",
    "                        'lr_scheduler': lr_scheduler[m]\n",
    "                    }\n",
    "\n",
    "    return bestHyperparameters, X_train, y_train, X_val, y_val, "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf96913-0c98-4916-9794-32293d592273",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81cc39e-05e5-4741-9e8f-77789b9d7d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds = [20, 30, 45, 70]\n",
    "\n",
    "n_categorical_features = 0\n",
    "n_numerical_features = 4\n",
    "n_static_features = n_categorical_features + n_numerical_features\n",
    "n_dynamic_features = 22\n",
    "n_timesteps = 14\n",
    "\n",
    "# Hyperparamas of network\n",
    "balance = True\n",
    "epochs = 10000\n",
    "batch_size = 128\n",
    "num_heads = 1\n",
    "\n",
    "\n",
    "layers = [3, 5, 8, 10, 15, 20, 25, 30, 35, 40, 50]\n",
    "lr_scheduler = [0.0001, 0.001, 0.01, 0.1]\n",
    "dropout_rate = [0.0, 0.1, 0.2, 0.3]\n",
    "\n",
    "w2 = 0.18\n",
    "w1 = 0.82\n",
    "\n",
    "tensor = True\n",
    "debug = True\n",
    "\n",
    "hyperparameters = {\n",
    "    \"n_categorical_features\": n_categorical_features,\n",
    "    \"n_numerical_features\": n_numerical_features,\n",
    "    \"n_static_features\": n_static_features,\n",
    "    \"n_dynamic_features\": n_dynamic_features,\n",
    "    \"n_timesteps\": n_timesteps,\n",
    "    \"num_heads\": num_heads,\n",
    "    \"w1\":w1, \"w2\":w2, \n",
    "\n",
    "    \"epochs\":epochs,\n",
    "    'batch_size': batch_size,\n",
    "    'maskValue':666,\n",
    "    'monitor': 'val_loss', \n",
    "    \"mindelta\": 0,\n",
    "    \"patience\":30,\n",
    "    'balance': balance,\n",
    "    'optimizer':'adam',\n",
    "    'kfold':5,\n",
    "    'level':3, \n",
    "    'verbose':0\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22bd45ca-0bfa-49de-9ef9-f8a25b01c778",
   "metadata": {},
   "source": [
    "# Generate predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5869f9ff-9c09-4f14-94a0-c5171e41d785",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "run_model = False\n",
    "if run_model:\n",
    "    v_early = []\n",
    "    loss_dev = []\n",
    "    v_models = []\n",
    "    bestHyperparameters_bySplit = {}\n",
    "    y_pred_by_split = {}\n",
    "\n",
    "    for i in range(1, 4):\n",
    "        X_test = pd.read_csv(\"../../../ORIGINAL_DATA/splits_14_days/bootstrap/split_\" + str(i) + \"/X_test_static.csv\",\n",
    "                                   index_col=0)\n",
    "        y_test = pd.read_csv(\"../../../ORIGINAL_DATA/splits_14_days/bootstrap/split_\" + str(i) + \"/y_test.csv\",\n",
    "                            index_col=0)\n",
    "    \n",
    "        bestHyperparameters, X_train, y_train, X_val, y_val = myCVGrid(hyperparameters,\n",
    "                                                                        dropout_rate,\n",
    "                                                                        lr_scheduler,\n",
    "                                                                        layers,\n",
    "                                                                        i,                                                              \n",
    "                                                                        seeds[i],\n",
    "                                                                        )\n",
    "        bestHyperparameters_bySplit[str(i)] = bestHyperparameters\n",
    "\n",
    "        # Save best hyperparameters for current split\n",
    "        split_directory = './Results_MLP-CIB/split_' + str(i)\n",
    "        if not os.path.exists(split_directory):\n",
    "            os.makedirs(split_directory)\n",
    "    \n",
    "        with open(os.path.join(split_directory, f\"bestHyperparameters_split_{i}.pkl\"), 'wb') as f:\n",
    "            pickle.dump(bestHyperparameters, f)\n",
    "    \n",
    "    \n",
    "        hyperparameters = {\n",
    "            \"n_categorical_features\": hyperparameters[\"n_categorical_features\"],\n",
    "            \"n_numerical_features\": hyperparameters[\"n_numerical_features\"],\n",
    "            \"n_static_features\": hyperparameters[\"n_static_features\"],\n",
    "            \"n_dynamic_features\": hyperparameters[\"n_dynamic_features\"],\n",
    "            \"w1\":hyperparameters[\"w1\"], \"w2\":hyperparameters[\"w2\"],                                    \n",
    "    \n",
    "            \"n_timesteps\": hyperparameters[\"n_timesteps\"],\n",
    "            'epochs':  hyperparameters[\"epochs\"],\n",
    "            \"num_heads\": num_heads,\n",
    "            'batch_size': hyperparameters[\"batch_size\"],\n",
    "            'maskValue': hyperparameters[\"maskValue\"],\n",
    "            'earlyStopping': True,\n",
    "            'kfold': hyperparameters[\"kfold\"],\n",
    "            'monitor': hyperparameters[\"monitor\"],\n",
    "            \"mindelta\": hyperparameters[\"mindelta\"],\n",
    "            \"patience\": hyperparameters[\"patience\"],\n",
    "            'balance': hyperparameters[\"balance\"],\n",
    "            \"dropout_rate\": bestHyperparameters[\"dropout_rate\"],\n",
    "            \"layers\": bestHyperparameters[\"layers\"],\n",
    "            \"lr_scheduler\": bestHyperparameters[\"lr_scheduler\"],\n",
    "            \"level\": 3, 'verbose': 0\n",
    "        }\n",
    "    \n",
    "\n",
    "        # Run the model \n",
    "        utils.reset_keras()\n",
    "        model, hist, early = run_network(\n",
    "            X_train,  y_train.individualMRGerm.values,\n",
    "            X_val, y_val.individualMRGerm.values,\n",
    "            hyperparameters, seeds[i]\n",
    "        )    \n",
    "\n",
    "        v_models.append(model)\n",
    "        loss_dev.append(hist.history['val_loss'])\n",
    "    \n",
    "        y_pred = model.predict(x=[X_test])\n",
    "        y_pred_by_split[str(i)] = y_pred\n",
    "        \n",
    "        with open(os.path.join(split_directory, f\"y_pred_split_{i}.pkl\"), 'wb') as f:\n",
    "            pickle.dump(y_pred, f)\n",
    "    \n",
    "        # Save model for current split\n",
    "        model_filename = os.path.join(split_directory, f\"model_split_{i}.h5\")\n",
    "        model.save(model_filename)\n",
    "    \n",
    "        # Calculate metrics\n",
    "        metrics_dict = utils.calculate_and_save_metrics(\n",
    "        y_test.individualMRGerm.values, \n",
    "        y_pred, \n",
    "        split_directory, \n",
    "        split_index=i\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b736b859-6656-4506-9625-38995d7cdf3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
